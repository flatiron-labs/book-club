# Chapter 1 - Introduction
## Historical Sysadmin approach
Team scales linearly with size and load of system
“Dev” & “Ops” divide. Fundamental tension between feature velocity & product stability

Do we exhibit any of these tensions?

## SRE
“what happens when you ask a software engineer to design an operations team”
tasked with writing software to replace previously manual “traditional ops” work
50% cap on “ops” work - tickets, on-call, manual tasks
Upper limit, over time should trend towards 0
Systems that are automatic vs automated

- What are some current examples of automatic vs automated in our day to day?
- Are some of these challenges specific to Google? How do cloud hosting providers play into this and the increase of SaaS dev tools? - Is there a different build vs buy mentality we should adopt vs a company like Google?

## SRE Tenets
Responsibilities
- Availability
- Latency
- Performance
- Efficiency
- Change management
- Monitoring
- Emergency response
- Capacity planning

Enduring focus on engineering work
Redirect excess operational work until reduced below 50% of time
Establishing an error budget
100% reliability is not the goal, law of diminishing returns. No difference between 100% & 99.999% reliability

Questions
- What’s our target reliability? How do we decide?
- Not a technical question, a product question
- What level of availability will the users be happy with, given how they use the product?
- What alternatives are available to users who are dissatisfied with the product’s availability?
- What happens to users’ usage of the product at different availability levels?

Balances ability to take risk vs stability by agreeing on an error budget that can be spent for risk and techniques such as canary rollouts
What are the different target availabilities & error budgets across our services? Are there differences between say Ironboard vs Registrar?

Monitoring

- Should not require a human to interpret, only when to take action
- Alerts - requires immediate human intervention to correct
- Tickets - human needs to take action in few days to avoid issues
- Logging - no action required, for diagnostics & forensics

Emergency Response
- MTTF - Mean time to Failure
- MTTR - Mean time to Repair/Resolve. Best measure of effective emergency response
- Playbook - produces 3x improvement in MTTR. record best practices and how to respond to alerts
- Emergency response exercises, “wheel of misfortune”

Change Management
- 70 % of outages are due to some kind of live change
- Implement progressive rollouts
- Quick & accurate problem detection
- Rolling back changes safely when problems arise
- How are we doing re: change management safety? How can CI/CD help?

Demand Forecasting
- Accurate organic demand forecasting
- Accurate incorporation of inorganic demand into planning
- How to ensure good communication about expected increases in load?
- Regular load testing
- Are we handicapped by DO being free?

Provisioning
- Must be conducted quickly and only when necessary
- Can be risky and treated with some level of caution
- Do AWS autoscaling / K8s reduce this risk?

Efficiency & Performance
- Meet capacity at specifically agreed upon response time

Google still appears to have a SRE vs SWE divide? Is that how we want to organize or do we face different challenges?
Do we infuse SRE tenets into our feature teams? Keep them separate?

# Chapter 2 - The Production Environment at Google
Overview of Google production environment
- Very specific to Google. What can we learn from them? Where do we rely on the expertise and maturity of cloud providers vs DIY?
- Borg (predecessor to K8s) - resource allocation / service scheduler.
- Jupiter - networking
- B4 - global backbone network

Abstract away hardware failures from both users and developer teams

BNS - Service discovery

- Storage - layered abstractions
- Lowest level - D. fileserver running on all machines in a cluster
- Colossus - cluster-wide filesystem abstraction
- Databases
- BigTable - scalable NoSQL
- Spanner - global SQL consistent store

Networking
- Geographic load balancing
- User service level load balancing
- Load balancing at RPC level (in the datacenter)
- Lock Service
- Paxos for distributed consensus across datacenters
- Monitoring and Alerting
- Borgman monitoring, scrapes metrics
- Software Infrastructure
- Stubby (gRPC)
- Data transferred via protobufs

Dev Environment 
- Single monorepo for all of Google
- Engineers can submit fixes for all components, even not their own

Shakespeare Service Example
- Many points of potential failure
- Rigorous testing, careful rollout, graceful degradation
- Load test to determine capacity and resource planning

## Chapter 3 - Embracing Risk

Extreme reliability comes at a cost. Limits how fast features can be developed.

SRE balances availability risks with goals of rapid innovation

### Managing Risk
- Cost of redundant resources to ensure highest availability
- Opportunity cost, engineers focused on reliability could be working on new features
- Make it “reliable enough”, (important to know business impact of availability & tolerance for unavailability), no more reliable than needed
- Interesting availability table to translate “nines” to actual allowed amount of downtime

Measuring Service Risk
- Identify objective metric to optimize reliability
- Time-based availability
- Not as useful when globally distributed & fault tolerant. Service may have a partial outage, it’s available somewhere.

Aggregate availability
- % of successful requests over a time period
- Not all requests created equal
- Failed sign up user request vs long term polling endpoint
-Metric may change based on type of service

User facing web requests
Batch / pipeline services
Most services have some metric that can be used to track success rate over time

Service level objectives
- Risk Tolerance
- Work with product owners to turn business goals into explicit reliability goals
- Consumer services (Search, Maps, Docs, etc)
-Product manager shape reliability requirements
- In absence of product management, engineers often shape this unknowingly

Questions to ask
- What level of availability is required?
- Do different types of failures have different effects on the service?
- How can we use the service cost to help locate a service on the risk continuum?
- What other service metrics are important to take into account?
- What level of service will the users expect?
- Does this service tie directly to revenue (either our revenue, or our customers’ revenue)?
- Is this a paid service, or is it free?
- If there are competitors in the marketplace, what level of service do those competitors provide?
- Is this service targeted at consumers, or at enterprises?

Types of Failure
- How resilient is the business to service downtime?
- Constant low rate of failure vs occasional full site outage?
- Schedule outages acceptable?
- Full failure mode better than partial (example of privacy/trust violations, bug exposes private data to unintended user)

Costs
- If we were to build and operate these systems at one more nine of availability, what would our incremental increase in revenue be?
- Does this additional revenue offset the cost of reaching that level of reliability?
- Other service metrics
- Different latency objectives / tradeoffs based on service audience & business requirements & objectives
- Risk Tolerance for Infrastructure Services
- Infrastructure tends to have multiple clients with varying needs
- Different levels of service for different tradeoffs
- Low Latency vs throughput, optimize for each use case
- Does this apply to us? How do cloud providers change this perspective?
- Motivation for Error Budgets
- Product development measured on feature velocity while SRE is measured on service reliability
Typical tensions:
- Software fault tolerance - how hardened is the product to unexpected events
- Testing - outages / privacy leaks vs moving too slow
- Push frequency - every push is risky, how much work to reduce that risk vs other work (feature)
- Canary duration & size - test changes on small subset of traffic. How much and for how long?
Tends to be informal, make it formal and agree on an objective between teams

Common incentive to balance innovation with reliability
Forming Your Error Budget
Product defines SLO
Establish error budget to meet SLO
As long as error budget remains, pushes and changes can be made

# Chapter 4 - Service Level Objectives
Impossiible to manage a service without understanding which behaviors matter for users
Define the following to deliver a specified level of service
- Service Level Indicators (SLI) - basic properties of metrics that matter,
- Service Level Objectives (SLO) - what values we want those metrics to have
- Service Level Agreements (SLA) - how we’ll react if we can’t provide the expected service
Gives confidence to knowing when a service is “healthy”

Indicators
- Common key SLIs
- Request latency
- Error rate
- System throughput
- Usually aggregated
- Availability / yield (time available / % of successful requests)
- Durability (for storage systems)

Objectives
- Range of acceptable values of SLIs
- Sets expectations with users on how the service will perform
- Without explicit SLO, users develop own beliefs on how reliable a service will/should be

Example: Global Chubby outages
- Highly reliable, but would cause global outages when down
- Too reliable, dependent services assumed would always be available
- Chubby SRE team intentionally takes down the service within the SLO / error budget to ensure users of the service have correct expectations about the availability of the service
- CHAOS ENGINEERING
- Agreements
- Consequences for missing SLOs
- Business & product decisions
- Some services (search for example) don’t have explicit SLAs with users, still good to define one internally as outages can still damage reputation and revenues
- In Practice
- Don’t pick too many SLIs to track, measure, and define SLOs for. Find the key ones
- What do you and your users care about?
- User-facing systems
- Availability
- Latency
- Throughput
- Storage systems
- Latency
- Availability
- Durability
- Big Data
- Throughput
- End to end latency
- ALL SERVICES
- Correctness
- Collecting Indicators
- Aggregating metrics
- Measurement window can obscure issues (instantaneous load vs average load)
- Percentile distributions better measurements than averages, less statistical fallacies
- Standardize Indicators
- Aggregation interval
- Aggregation regions
- Measurement frequency
- Which requests
- How it’s collected / gathered
- Data access latency
- Often what your users care about is difficult to truly measure, you’ll need to approximate in some way
- Don’t start with what’s easy to measure (SLIs first). Start with SLOs and work backwards to find the corresponding SLIs
- Choosing targets
- Keep it simple
- Don’t pick a target based on current performance
- Avoid absolutes
- As few SLOs as possible
- Perfection can wait
- What needs to happen to meet the target?
- Keep a safety margin
- Don’t overachieve

Our WIP Service Level doc: https://docs.google.com/spreadsheets/d/1qPXUHTobzHsydDnXf7bQmp7ZZZUpmBePHN5Sycqmbuw/edit?usp=sharing


