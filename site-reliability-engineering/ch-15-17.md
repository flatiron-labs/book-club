# Site Reliability Engineering 

## Chapter 15: Postmortem Culture: Learning from Failure

> A postmortem is a written record of an incident, its impact, the actions taken to mitigate or resolve it, the root cause(s), and the follow-up actions to prevent the incident from recurring. 

### Google's Postmortem Philosophy
> Writing a postmortem is not punishment—it is a learning opportunity for the entire company.

But they are costly! 

Here is when you might post mortem:
- user-visible downtime or significant degradation
- data loss
- on-call eng intervention
- resolution time beyond threshold
- monitoring failure 

Postmortems should be blameless! Focus on the causes without indicting individuals or teams. 

### Collaborate and Share Knowledge
- We should add lessons learned to our template!
- Review criteria are interesting. "Was the root cause sufficiently deep?"
- Should share postmortems with the largest possible audience. 

### Introducing a Postmortem Culture
- Postmortem of the month
- Postmortem reading clubs (LOL)
- Wheel of Misfortune 

### Discussion Questions
1. What are some ways we can keep postmortems truly blameless? 
2. What did you think about the postmortem template?
3. How else can we be sharing our postmortems out with larger audiences? How else can we celebrate postmortems? 

## Chapter 16: Tracking Outages

Google's "Outalator" aggregates all alerts in order to annotate, group, and analyze data.

Postmortems are only part of the picture, for individual outages. What about the bigger picture?

Google's "Escalator" is a central replicated system that tracks whether human has acknowledged notification, then escalates to next if necessary.

Google's "Outalator" displays list of notifications for multiple queues at once. Grouping functionality, annotations, tagging. 

### Discussion Questions
1. Tagging alerts could be cool. How can we make that happen? We're doing something like this for support tickets. 
2. What kind of stats can we pull on outages? How are we keeping track?

## Chapter 17: Testing for Reliability

SRE's quantify confidence in the systems they maintain. 

> The amount of testing you need to conduct depends on the reliability requirements for your system. As the percentage of your codebase covered by tests increases, you reduce uncertainty and the potential decrease in reliability from each change

### Relationships Between Testing and Mean Time to Repair
> Passing a test or a series of tests doesn’t necessarily prove reliability. However, tests that are failing generally prove the absence of reliability.

Mean Time to Repair (MTTR): how long it takes to fix the bug through rollback or other action

Zero MTTR: when system-levle test is applied to subsystem and test detects problem monitoring would detect. Code is blocked, so bug never makes it to prod. 

### Types of Software Testing

*Traditional Tests*
- System Tests: large scale test for undeployed system. smoke tests, performance tests, regression tests. 
- Integration Tests: an assembled component, dependency injection will help 
- Unit Tests: smallest and simplest, class or function 

Tests are expensive tho, so be mindful of these costs to developer productivity.

*Production Tests*
- Do. It. Live.
- Rollouts entangle tests. They happen in stages. 
- Configuration tests. 
- Stress tests: how full can a database get before writes start to fail? how many queries a second before overload? 

*Canary test*: "canary in a coal mine" refers to practice of using live bird to detect toxic gases before humans poisoned
- subset of servers upgraded to new config and then incubated
- release continues if good, reverted if bad
- incubation period called "baking the binary"

> In many cases, individual components don’t gracefully degrade beyond a certain point—instead, they catastrophically fail.

### Creating a Test and Build Environment
How to test when you're productionizing a prototype

- Can you prioritize the codebase in any way?
> if every task is high priority, none of the tasks are high priority.

- Are any functions or classes mission- or business-critical?

- Which APIs are other teams using? 

> Shipping software that is obviously broken is among the most cardinal sins of a developer. It takes little effort to create a series of smoke tests to run for every release.

To start strong test culture, start documenting all bugs as test cases. 

> Life-critical or revenue-critical systems demand substantially higher levels of test quality and coverage than a non-production script with a short shelf life.

### Testing at Scale
> Software that bypasses the usual heavily tested API (even if it does so for a good cause) could wreak havoc on a live service.
(aka ssh'ing into prod box)



