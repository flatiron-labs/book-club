# 5. Eliminating Toil
Toil  Defined
- Administrative work = overhead
- Meetings, goal setting, paperwork etc
- Operational work = toil
- Work tied to running a production service

## Attributes of toil
- Manual i.e running a script
- Repetitive: More than just once or twice
- Automatable: Task done by humans where human judgment isn’t necessary
- Tactical: Interrupt-driven and reactive, i.e. pager alerts
- No enduring value: service remains in the same state after you have finished a task and that scales linearly as a service grows
As service size grow → task size grows
- Did you have previous classifications of toil?
- Are there any tasks that jump out to you that satisfy more than one of these attributes?

## Why Less Toil is Better
- Engineering work of the SRE  is what allows for reducing toil and scaling up services
- SRE goal to keeping operational work (i.e., toil) below 50% of each SRE’s time
- Majority of time being spent on feature work or minimizing future toil
- Feature development typically focuses on improving reliability, performance, or utilization, which often reduces toil as a second-order effect
- Examples  of  feature work that greatly reduced some toil?
- Increased toil?
- Do we currently split toil work among feature squads and feature work to the devops team?
- Calculating Toil
- SREs are primary on-call (1 week), second on-call (1-week) out of a 6 week rotation = 33% toil
- Sources of toil
- Top source of toil is interrupts (that is, non-urgent service-related messages and emails)
- Urgent Response
- Releases and pushes

- What’s our current toil percentage from being on call? 
- What are our top source of toil?
- How can we reduce this?
- Engineering work
- Novel and intrinsically requires human judgment. It produces a permanent improvement in your service, and is guided by a strategy

SRE Activities
Software engineering
Systems engineering (includes consulting on architecture, design)

Toil
Overhead
Is Toil Always Bad?
 - Can be low-risk and low-stress and serve as quick wins
- Toil is unavoidable in any12  engineering role
- Little toil = ok; Lots of toil = bad
- Too much toil can lead to career stagnation and low morale

Spending too much time on toil:
- Creates confusion
- Slows progress
- Sets a precedent
- Promotes attrition
- Causes breach of faith
- Motivational Quote of the Day: invent more, and toil less.

# 6. Monitoring Distributed Systems
What pages should actually wake you up at 2am? 

## Glossary
- Monitoring - collecting/processing/aggregating/ displaying data about a system (i.e.  query/error counts)
- White-box monitoring - metrics based monitoring (i.e. logs, http handler)
- Black-box monitoring - testing externally visible behavior (qa?)
- Dashboard - application that provides a visual summary of  core metrics (Datadog) or team info such as ticket queue (jira)
- Alert - notification intended to be read by a human (tickets,  emails, pages)
- Root Cause - defect in the system or software that can be fixed, eliminating the current problem
- Node and Machinery -  Single instance of a running kernel
- Push - changes to a  services (software or config)

## Why Monitoring? 
- Analyzing long-term trends
- Comparing over time or experiment groups
- Alerting
- Building dashboards
- Conducting ad hoc retrospective analysis (i.e., debugging)
- Relying on paging is expensive and monitoring can help by telling us what’s broken or about to break
- Effective alerting systems have good signal and very low noise
- Setting Reasonable  Expectations for Monitoring
- a Google SRE team with 10–12 members typically has one or sometimes two members whose primary assignment is to build and maintain monitoring systems for their service
- avoid "magic" systems that try to learn thresholds or automatically detect causality
- If I know the database is slow, alert for a slow database; otherwise, alert for the website being generally slow as opposed to dependency-reliant rules usually pertain to very stable parts of our system “If a datacenter is drained, then don’t alert me on its latency”
- How do we structure our monitoring rules?

## Symptoms versus Causes
- what’s broken (symptom, and why(cause)?
- Determining the difference between the two allows for better monitoring with less noise, more signal
- How many issues trigger an alert when github goes down? What are the side effects
- Black-box versus White-Box
- Black-box monitoring is symptom-oriented and represents active—not predicted—problems
- White-box monitoring allows detection of imminent problems
- One person’s symptom is another person’s cause; white-box monitoring can sometimes be oriented to  cause or symptom 
- White box helps with debugging; black-box enforces discipline of alerting for problems that are already ongoing and contributing to real symptoms
- How do we normally orient our monitors? 

## The Four Golden Signals
- Latency: The time it takes to service a request
- Traffic: Meaure of how much demand is being placed on your system;  i.e. # http requests per second
- Errors: Rate of requests that fail explicitly or implicitly 
- Saturation: “Fullness”  of your service

## Worrying About Your Tail
- Creating monitors by the mean of some quantity can be dangerous and often misleading 
- Distributing the histogram boundaries approximately exponentially (in this case by factors of roughly 3) is often an easy way to visualize the distribution of your requests.
- Choosing and Appropriate Resolution for measurements
- Different aspects of a system should be measured with different levels of granularity
- Collection and retention of measurements can come at a high cost so take care of how you structure the granularity of your monitors

## As Simple As Possible, No Simpler
- Satisfying all these requirements  can lead to overly complex monitoring systems  
- Guidelines for choosing what to monitor
- The rules that catch real incidents most often should be as simple, predictable, and reliable as possible.
- Data collection, aggregation, and alerting configuration that is rarely exercised (e.g., less than once a quarter for some SRE teams) should be up for removal.
- Signals that are collected, but not exposed in any prebaked dashboard nor used by any alert, are candidates for removal.

## Tying These Principles together 
- Does this rule detect an otherwise undetected condition that is urgent, actionable, and actively or imminently user-visible?
- Will I ever be able to ignore this alert, knowing it’s benign? When and why will I be able to ignore this alert, and how can I avoid this scenario?
- Does this alert definitely indicate that users are being negatively affected? Are there detectable cases in which users aren’t being negatively impacted, such as drained traffic or test deployments, that should be filtered out?
- Can I take action in response to this alert? Is that action urgent, or could it wait until morning? Could the action be safely automated? Will that action be a long-term fix, or just a short-term workaround?
- Are other people getting paged for this issue, therefore rendering at least one of the pages unnecessary?
- What questions or guidelines do we currently use when creating monitors?
- Fundamental pager philosophy
- Every time the pager goes off, I should be able to react with a sense of urgency. I can only react with a sense of urgency a few times a day before I become fatigued.
- Every page should be actionable.
- Every page response should require intelligence. If a page merely merits a robotic response, it shouldn’t be a page.
- Pages should be about a novel problem or an event that hasn’t been seen before.
- Monitoring for the Long Term
- Systems are ever evolving and changing
- An alert that was once rare but now occurs frequently warrants determining the root  cause  an eliminating the problem
- Every page that happens today distracts a human from improving the system for tomorrow

## Bigtable SRE: A tale of Over-Alerting - Backing off of non urgent alerts provides space for longer term problems
- Gmail: Predictable, Scriptable Responses from Human

# 7. The Evolution of Automation at Google
## The Value of Automation
- Consistency: None automated processes are not  going to be executed the same way every time. This inconsistency leads to mistakes, oversights, issues with data quality, reliability problems
- A Platform: Automation allows for expansion/extension: results in a platform that can be applied to more systems
- Centralizes mistakes; fix occurs once
- Faster Repairs: Use automation  to resolve common faults in a system
- Faster Action: Humans != machines and are not as fast
- Time Saving: If you automate a task, anyone can execute that task saving time
- Decouple operator from operation

## The Value for Google SRE
Google has a strong bias toward automation and relies on it. They’ve crossed the threshold of the processes being manageable manually
For such a large system, consistency, quickness and reliability are at the core of conversation

Google avoids: 
- equipment without a readily accessible API, software for which no source code is available, or another impediment to complete control over production operations
- Own the product in production
- Don’t always have the time for automation and it might not be appropriate
- Platform-based approach as necessary for manageability and scalability
- Build over buy? When is it appropriate
- How do we developea more platform-based approach?

## The Use Cases for Automation
Examples:
User account creation, cluster turnups, changes to dependencies
Google SRE’s Use Cases
primary affinity to manage lifecycles of a system; not managing quality of data
High level abstractions are easier to reason about but can lead to problems during failures

## A Hierarchy of Automation Classes
- No glue logic
- Evolution
- No automation
- Externally Maintained system specific automation
- Externally maintained generic automation
- Internally maintained system-specific automation
- Systems that don’t need any automation
- Sometimes manual automation is unavioidable

## Case studies
- Automate yourself out of a job
- migrating MySQL onto Google’s cluster scheduling system, Borg to eliminate the need for replica and maintenance, and can spin up multiple instances on 1 physical machine
- Tasks move on borg - failover increased on masters
- Took a long time until they automated failover with Decider
- So much free time and joy
- Moral: create a platform not just replace existing manual proceses
- Soothing the Pain: Applying Automation to Cluster Turn ups
- New hires =  new cluster
- New flags +  new components each week
- Bigtable didn’t use first disk( intentionally ) but automation assumed it would
- Don’t rely on implicit safety systems
- Welcome, Prodtest - unit testing real world services
- Use the tests to fix the issues
- Automation varies on Competence, Latency, Relevance
- Product development teams should have some operational awareness and stake in automation
- Borg: Birth of Warehouse-Scale Computer
- Reliability is the fundamental feature
- Having heavy automation relieves human contact with the system; when things go wrong, humans aren’t able to effectively operate on the system for lack of practice
- Nonautonomous systems- automation replaces human action with the idea that human action is still possible
- Recommendations
- You don’t need to  be at google’s scale to start automating
- Retrofitting automation to a system is difficult and the highest leverage comes in during design phase
- Enabling Failure at scale
- Once upon a time, automation caused all the colos at google to get wiped?




